{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q ultralytics>=8.3.0 roboflow wandb huggingface_hub pyyaml pillow"
      ],
      "metadata": {
        "id": "tDaag3LpcHxh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "WANDB_API_KEY = userdata.get('wandb')\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "ROBOFLOW_API_KEY = userdata.get('roboflow')"
      ],
      "metadata": {
        "id": "pGCyOUBHe1yK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2Ô∏è‚É£ Load Secrets & Login\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Load secrets from Colab\n",
        "WANDB_API_KEY = userdata.get('wandb')\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "ROBOFLOW_API_KEY = userdata.get('roboflow')  # Add this to your secrets!\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['WANDB_API_KEY'] = WANDB_API_KEY\n",
        "os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "\n",
        "# Login to services\n",
        "import wandb\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "\n",
        "from huggingface_hub import login as hf_login\n",
        "hf_login(token=HF_TOKEN)\n",
        "\n",
        "print(\"‚úÖ Logged in to W&B and HuggingFace\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWobrUpne1tg",
        "outputId": "23d12244-79c8-4f0b-f5eb-2466e1fd1f36"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshng2025\u001b[0m (\u001b[33mImperial-College-London-SPQR\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Logged in to W&B and HuggingFace\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3Ô∏è‚É£ Check GPU & Test YOLO12\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Test YOLO12\n",
        "from ultralytics import YOLO\n",
        "test_model = YOLO(\"yolo12n.pt\")\n",
        "print(\"\\n‚úÖ YOLO12 loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MIi8OVCe1pM",
        "outputId": "875cd9d3-0c50-404a-af82-fd6670b07f7e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.9.0+cu126\n",
            "CUDA: True\n",
            "GPU: NVIDIA L4\n",
            "Memory: 23.8 GB\n",
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo12n.pt to 'yolo12n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.3MB 257.8MB/s 0.0s\n",
            "\n",
            "‚úÖ YOLO12 loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4Ô∏è‚É£ Configuration\n",
        "\n",
        "# =============================================================================\n",
        "# EDIT THESE SETTINGS\n",
        "# =============================================================================\n",
        "\n",
        "# Model: yolo12n (fast) | yolo12s | yolo12m (balanced) | yolo12l | yolo12x (accurate)\n",
        "MODEL = \"yolo12m.pt\"\n",
        "\n",
        "# Training\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 16  # Reduce to 8 if OOM\n",
        "IMG_SIZE = 640\n",
        "\n",
        "# Dataset (Roboflow)\n",
        "WORKSPACE = \"shahed136\"\n",
        "PROJECT = \"shahed136-detect\"\n",
        "VERSION = 1\n",
        "\n",
        "# Logging\n",
        "WANDB_ENTITY = \"Imperial-College-London-SPQR\"\n",
        "WANDB_PROJECT = \"European-Defense-Hackathon-Warsaw\"\n",
        "HF_REPO = \"shng2025/EDTH-Warsaw-shahed136-detector\"\n",
        "\n",
        "print(f\"\"\"\n",
        "Configuration:\n",
        "  Model: {MODEL}\n",
        "  Epochs: {EPOCHS}\n",
        "  Batch: {BATCH_SIZE}\n",
        "  Image Size: {IMG_SIZE}\n",
        "  W&B: {WANDB_ENTITY}/{WANDB_PROJECT}\n",
        "  HuggingFace: {HF_REPO}\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYZqlrJxgSAh",
        "outputId": "76683536-b366-4adf-d14c-090db624f55f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configuration:\n",
            "  Model: yolo12m.pt\n",
            "  Epochs: 1\n",
            "  Batch: 16\n",
            "  Image Size: 640\n",
            "  W&B: Imperial-College-London-SPQR/European-Defense-Hackathon-Warsaw\n",
            "  HuggingFace: shng2025/EDTH-Warsaw-shahed136-detector\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5Ô∏è‚É£ Download Dataset\n",
        "from roboflow import Roboflow\n",
        "import yaml\n",
        "\n",
        "rf = Roboflow(api_key=ROBOFLOW_API_KEY)  # Already loaded from secrets\n",
        "project = rf.workspace(\"edthwarsaw\").project(\"shahed136-detect-emoo1\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n",
        "DATASET_PATH = dataset.location\n",
        "DATA_YAML = f\"{DATASET_PATH}/data.yaml\"\n",
        "\n",
        "with open(DATA_YAML, 'r') as f:\n",
        "    data_info = yaml.safe_load(f)\n",
        "CLASS_NAMES = data_info.get(\"names\", [])\n",
        "if isinstance(CLASS_NAMES, dict):\n",
        "    CLASS_NAMES = [CLASS_NAMES[i] for i in sorted(CLASS_NAMES.keys())]\n",
        "\n",
        "print(f\"‚úÖ Dataset: {DATASET_PATH}\")\n",
        "print(f\"Classes: {CLASS_NAMES}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7GV1mMAkOFW",
        "outputId": "2abf6749-8ab2-40ba-efea-60f496f7dbd9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in shahed136-detect-1 to yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 207071/207071 [00:12<00:00, 16305.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to shahed136-detect-1 in yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16246/16246 [00:01<00:00, 10082.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset: /content/shahed136-detect-1\n",
            "Classes: ['1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6Ô∏è‚É£ Setup Logging Classes\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "class WandBLogger:\n",
        "    \"\"\"W&B logger with visual predictions.\"\"\"\n",
        "\n",
        "    def __init__(self, run_name, config):\n",
        "        import wandb\n",
        "        self.wandb = wandb\n",
        "        self.run = wandb.init(\n",
        "            entity=WANDB_ENTITY,\n",
        "            project=WANDB_PROJECT,\n",
        "            name=run_name,\n",
        "            tags=[\"yolo12\", \"drone-detection\", \"shahed-136\", \"defense\"],\n",
        "            config=config,\n",
        "        )\n",
        "        self.run_id = self.run.id\n",
        "        print(f\"[W&B] {self.run.url}\")\n",
        "\n",
        "    def log(self, metrics, step=None):\n",
        "        self.wandb.log(metrics, step=step)\n",
        "\n",
        "    def log_predictions(self, model_path, images, class_names, epoch):\n",
        "        \"\"\"Log detection visualizations.\"\"\"\n",
        "        # Load model fresh to use YOLO wrapper's predict\n",
        "        temp_model = YOLO(model_path)\n",
        "        results = temp_model.predict(images, conf=0.25, verbose=False)\n",
        "        wandb_images = []\n",
        "\n",
        "        for img_path, result in zip(images, results):\n",
        "            img = Image.open(img_path)\n",
        "            boxes_data = []\n",
        "\n",
        "            if result.boxes is not None:\n",
        "                for box in result.boxes:\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                    conf = float(box.conf[0])\n",
        "                    cls_id = int(box.cls[0])\n",
        "                    cls_name = class_names[cls_id] if cls_id < len(class_names) else str(cls_id)\n",
        "                    boxes_data.append({\n",
        "                        \"position\": {\n",
        "                            \"minX\": float(x1)/img.width, \"minY\": float(y1)/img.height,\n",
        "                            \"maxX\": float(x2)/img.width, \"maxY\": float(y2)/img.height,\n",
        "                        },\n",
        "                        \"class_id\": cls_id,\n",
        "                        \"box_caption\": f\"{cls_name}: {conf:.2f}\",\n",
        "                        \"scores\": {\"confidence\": conf},\n",
        "                    })\n",
        "\n",
        "            wandb_images.append(self.wandb.Image(\n",
        "                img,\n",
        "                boxes={\"predictions\": {\n",
        "                    \"box_data\": boxes_data,\n",
        "                    \"class_labels\": {i: n for i, n in enumerate(class_names)},\n",
        "                }},\n",
        "                caption=f\"Epoch {epoch}\",\n",
        "            ))\n",
        "\n",
        "        self.wandb.log({\"predictions/samples\": wandb_images}, step=epoch)\n",
        "\n",
        "    def log_gt_vs_pred(self, model_path, img_paths, label_paths, class_names, epoch):\n",
        "        \"\"\"Log ground truth vs predictions.\"\"\"\n",
        "        temp_model = YOLO(model_path)\n",
        "        images = []\n",
        "        for img_path, label_path in zip(img_paths, label_paths):\n",
        "            img = Image.open(img_path)\n",
        "            w, h = img.size\n",
        "\n",
        "            # Ground truth\n",
        "            gt_boxes = []\n",
        "            if Path(label_path).exists():\n",
        "                with open(label_path) as f:\n",
        "                    for line in f:\n",
        "                        parts = line.strip().split()\n",
        "                        if len(parts) >= 5:\n",
        "                            cls_id = int(parts[0])\n",
        "                            xc, yc, bw, bh = map(float, parts[1:5])\n",
        "                            gt_boxes.append({\n",
        "                                \"position\": {\"minX\": xc-bw/2, \"minY\": yc-bh/2, \"maxX\": xc+bw/2, \"maxY\": yc+bh/2},\n",
        "                                \"class_id\": cls_id,\n",
        "                                \"box_caption\": f\"GT: {class_names[cls_id] if cls_id < len(class_names) else cls_id}\",\n",
        "                            })\n",
        "\n",
        "            # Predictions\n",
        "            result = temp_model.predict(img_path, conf=0.25, verbose=False)[0]\n",
        "            pred_boxes = []\n",
        "            if result.boxes is not None:\n",
        "                for box in result.boxes:\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                    conf = float(box.conf[0])\n",
        "                    cls_id = int(box.cls[0])\n",
        "                    pred_boxes.append({\n",
        "                        \"position\": {\"minX\": x1/w, \"minY\": y1/h, \"maxX\": x2/w, \"maxY\": y2/h},\n",
        "                        \"class_id\": cls_id,\n",
        "                        \"box_caption\": f\"Pred: {class_names[cls_id] if cls_id < len(class_names) else cls_id} ({conf:.2f})\",\n",
        "                        \"scores\": {\"confidence\": conf},\n",
        "                    })\n",
        "\n",
        "            labels = {i: n for i, n in enumerate(class_names)}\n",
        "            images.append(self.wandb.Image(img, boxes={\n",
        "                \"ground_truth\": {\"box_data\": gt_boxes, \"class_labels\": labels},\n",
        "                \"predictions\": {\"box_data\": pred_boxes, \"class_labels\": labels},\n",
        "            }))\n",
        "\n",
        "        self.wandb.log({\"labeling_quality/comparison\": images}, step=epoch)\n",
        "\n",
        "    def log_label_distribution(self, dataset_path, class_names):\n",
        "        \"\"\"Log class distribution.\"\"\"\n",
        "        counts = {}\n",
        "        for split in [\"train\", \"valid\", \"val\", \"test\"]:\n",
        "            labels_dir = Path(dataset_path) / split / \"labels\"\n",
        "            if not labels_dir.exists():\n",
        "                continue\n",
        "            for f in labels_dir.glob(\"*.txt\"):\n",
        "                for line in open(f):\n",
        "                    parts = line.strip().split()\n",
        "                    if parts:\n",
        "                        cls_id = int(parts[0])\n",
        "                        name = class_names[cls_id] if cls_id < len(class_names) else str(cls_id)\n",
        "                        counts[name] = counts.get(name, 0) + 1\n",
        "\n",
        "        table = self.wandb.Table(data=[[k, v] for k, v in counts.items()], columns=[\"class\", \"count\"])\n",
        "        self.wandb.log({\"dataset/label_distribution\": self.wandb.plot.bar(table, \"class\", \"count\", title=\"Labels\")})\n",
        "        print(f\"[W&B] Label counts: {counts}\")\n",
        "\n",
        "    def finish(self):\n",
        "        self.wandb.finish()\n",
        "\n",
        "\n",
        "class HFCheckpointer:\n",
        "    \"\"\"HuggingFace Hub checkpointing.\"\"\"\n",
        "\n",
        "    def __init__(self, run_name):\n",
        "        from huggingface_hub import HfApi, create_repo\n",
        "        self.api = HfApi()\n",
        "        self.run_name = run_name\n",
        "        self.run_folder = f\"runs/{run_name}\"\n",
        "\n",
        "        try:\n",
        "            create_repo(HF_REPO, private=True, exist_ok=True, repo_type=\"model\")\n",
        "            print(f\"[HF] https://huggingface.co/{HF_REPO}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[HF] {e}\")\n",
        "\n",
        "    def upload(self, model_path, epoch, metrics=None, is_best=False):\n",
        "        from huggingface_hub import upload_file\n",
        "\n",
        "        if not Path(model_path).exists():\n",
        "            return\n",
        "\n",
        "        filename = f\"{self.run_folder}/best.pt\" if is_best else f\"{self.run_folder}/epoch_{epoch:04d}.pt\"\n",
        "\n",
        "        try:\n",
        "            upload_file(model_path, filename, HF_REPO, commit_message=f\"Epoch {epoch}\" + (\" (best)\" if is_best else \"\"))\n",
        "            print(f\"[HF] Uploaded: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[HF] Upload failed: {e}\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Logging classes ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18DmBIKNgR-L",
        "outputId": "988c0434-5592-43f0-f6a0-0819c4413491"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Logging classes ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7Ô∏è‚É£ üöÄ Train YOLO12\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.utils import SETTINGS\n",
        "\n",
        "SETTINGS[\"wandb\"] = True\n",
        "\n",
        "# Generate run name\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_name = Path(MODEL).stem\n",
        "run_name = f\"{model_name}_{timestamp}\"\n",
        "print(f\"Run: {run_name}\")\n",
        "\n",
        "# Initialize loggers\n",
        "config = {\"model\": MODEL, \"epochs\": EPOCHS, \"batch\": BATCH_SIZE, \"img_size\": IMG_SIZE}\n",
        "wandb_logger = WandBLogger(run_name, config)\n",
        "hf_checkpointer = HFCheckpointer(run_name)\n",
        "\n",
        "# Log dataset stats\n",
        "wandb_logger.log_label_distribution(DATASET_PATH, CLASS_NAMES)\n",
        "\n",
        "# Get sample images for visualization\n",
        "val_dir = Path(DATASET_PATH) / \"valid\" / \"images\"\n",
        "if not val_dir.exists():\n",
        "    val_dir = Path(DATASET_PATH) / \"val\" / \"images\"\n",
        "\n",
        "sample_images = [str(p) for p in list(val_dir.glob(\"*.jpg\"))[:16]]\n",
        "\n",
        "# Get GT-pred pairs\n",
        "labels_dir = val_dir.parent / \"labels\"\n",
        "pairs = []\n",
        "for img in val_dir.glob(\"*\"):\n",
        "    if img.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "        lbl = labels_dir / f\"{img.stem}.txt\"\n",
        "        if lbl.exists():\n",
        "            pairs.append((str(img), str(lbl)))\n",
        "pairs = pairs[:8]\n",
        "\n",
        "# Load model\n",
        "model = YOLO(MODEL)\n",
        "\n",
        "# Callbacks\n",
        "best_map = [0.0]\n",
        "\n",
        "def on_epoch_end(trainer):\n",
        "    epoch = trainer.epoch\n",
        "    metrics = trainer.metrics\n",
        "\n",
        "    # Log metrics\n",
        "    wandb_metrics = {\n",
        "        \"train/box_loss\": metrics.get(\"train/box_loss\", 0),\n",
        "        \"train/cls_loss\": metrics.get(\"train/cls_loss\", 0),\n",
        "        \"metrics/mAP50\": metrics.get(\"metrics/mAP50(B)\", 0),\n",
        "        \"metrics/mAP50-95\": metrics.get(\"metrics/mAP50-95(B)\", 0),\n",
        "        \"metrics/precision\": metrics.get(\"metrics/precision(B)\", 0),\n",
        "        \"metrics/recall\": metrics.get(\"metrics/recall(B)\", 0),\n",
        "    }\n",
        "    wandb_logger.log(wandb_metrics, step=epoch)\n",
        "\n",
        "    # Visual predictions every 5 epochs\n",
        "    if epoch % 5 == 0 and sample_images:\n",
        "        # Use last.pt for predictions\n",
        "        weights_path = Path(trainer.save_dir) / \"weights\" / \"last.pt\"\n",
        "        if weights_path.exists():\n",
        "            wandb_logger.log_predictions(str(weights_path), sample_images, CLASS_NAMES, epoch)\n",
        "            if pairs:\n",
        "                wandb_logger.log_gt_vs_pred(str(weights_path), [p[0] for p in pairs], [p[1] for p in pairs], CLASS_NAMES, epoch)\n",
        "\n",
        "    # HuggingFace checkpoints\n",
        "    current_map = metrics.get(\"metrics/mAP50(B)\", 0)\n",
        "    is_best = current_map > best_map[0]\n",
        "    if is_best:\n",
        "        best_map[0] = current_map\n",
        "\n",
        "    if epoch % 10 == 0 or is_best:\n",
        "        save_dir = trainer.save_dir\n",
        "        last_pt = Path(save_dir) / \"weights\" / \"last.pt\"\n",
        "        best_pt = Path(save_dir) / \"weights\" / \"best.pt\"\n",
        "\n",
        "        if last_pt.exists():\n",
        "            hf_checkpointer.upload(str(last_pt), epoch)\n",
        "        if is_best and best_pt.exists():\n",
        "            hf_checkpointer.upload(str(best_pt), epoch, is_best=True)\n",
        "\n",
        "model.add_callback(\"on_train_epoch_end\", on_epoch_end)\n",
        "\n",
        "# TRAIN!\n",
        "results = model.train(\n",
        "    data=DATA_YAML,\n",
        "    epochs=EPOCHS,\n",
        "    batch=BATCH_SIZE,\n",
        "    imgsz=IMG_SIZE,\n",
        "    patience=20,\n",
        "\n",
        "    optimizer=\"AdamW\",\n",
        "    lr0=0.001,\n",
        "    lrf=0.01,\n",
        "    weight_decay=0.0005,\n",
        "\n",
        "    augment=True,\n",
        "    mosaic=1.0,\n",
        "\n",
        "    project=\"runs/detect\",\n",
        "    name=run_name,\n",
        "    exist_ok=True,\n",
        "    save_period=10,\n",
        "\n",
        "    device=0,\n",
        "    workers=4,\n",
        "    amp=True,\n",
        "\n",
        "    plots=True,\n",
        "    save=True,\n",
        "    val=True,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "wandb_logger.finish()\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "print(f\"Results: runs/detect/{run_name}\")\n",
        "print(f\"Best weights: runs/detect/{run_name}/weights/best.pt\")\n",
        "print(f\"W&B: https://wandb.ai/{WANDB_ENTITY}/{WANDB_PROJECT}\")\n",
        "print(f\"HuggingFace: https://huggingface.co/{HF_REPO}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ckm_7GsSgR7n",
        "outputId": "7c7bef06-4894-4f91-b320-d4a0420f9368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: yolo12m_20251206_144537\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251206_144537-rdddd6u2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/Imperial-College-London-SPQR/European-Defense-Hackathon-Warsaw/runs/rdddd6u2' target=\"_blank\">yolo12m_20251206_144537</a></strong> to <a href='https://wandb.ai/Imperial-College-London-SPQR/European-Defense-Hackathon-Warsaw' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/Imperial-College-London-SPQR/European-Defense-Hackathon-Warsaw' target=\"_blank\">https://wandb.ai/Imperial-College-London-SPQR/European-Defense-Hackathon-Warsaw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/Imperial-College-London-SPQR/European-Defense-Hackathon-Warsaw/runs/rdddd6u2' target=\"_blank\">https://wandb.ai/Imperial-College-London-SPQR/European-Defense-Hackathon-Warsaw/runs/rdddd6u2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W&B] https://wandb.ai/Imperial-College-London-SPQR/European-Defense-Hackathon-Warsaw/runs/rdddd6u2\n",
            "[HF] https://huggingface.co/shng2025/EDTH-Warsaw-shahed136-detector\n",
            "[W&B] Label counts: {'1': 8117}\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo12m.pt to 'yolo12m.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 39.0MB 42.6MB/s 0.9s\n",
            "Ultralytics 8.3.235 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/shahed136-detect-1/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolo12m_20251206_144537, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/yolo12m_20251206_144537, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 94.7MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
            "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
            "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  6                  -1  2   2689536  ultralytics.nn.modules.block.A2C2f           [512, 512, 2, True, 4]        \n",
            "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  8                  -1  2   2689536  ultralytics.nn.modules.block.A2C2f           [512, 512, 2, True, 1]        \n",
            "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 11                  -1  1   1248768  ultralytics.nn.modules.block.A2C2f           [1024, 512, 1, False, -1]     \n",
            " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 14                  -1  1    378624  ultralytics.nn.modules.block.A2C2f           [1024, 256, 1, False, -1]     \n",
            " 15                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 17                  -1  1   1183232  ultralytics.nn.modules.block.A2C2f           [768, 512, 1, False, -1]      \n",
            " 18                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 20                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
            " 21        [14, 17, 20]  1   1411795  ultralytics.nn.modules.head.Detect           [1, [256, 512, 512]]          \n",
            "YOLOv12m summary: 292 layers, 20,138,259 parameters, 20,138,243 gradients, 67.7 GFLOPs\n",
            "\n",
            "Transferred 745/751 items from pretrained weights\n",
            "Freezing layer 'model.21.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.4MB 314.4MB/s 0.0s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1001.3¬±532.7 MB/s, size: 26.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/shahed136-detect-1/train/labels... 6722 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6722/6722 1.6Kit/s 4.3s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/shahed136-detect-1/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 542.5¬±234.3 MB/s, size: 27.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/shahed136-detect-1/valid/labels... 930 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 930/930 1.2Kit/s 0.8s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/shahed136-detect-1/valid/labels.cache\n",
            "Plotting labels to /content/runs/detect/yolo12m_20251206_144537/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 123 weight(decay=0.0), 130 weight(decay=0.0005), 129 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/yolo12m_20251206_144537\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        1/1      11.2G     0.5894     0.7059     0.9723         37        640: 46% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 192/421 1.8it/s 1:55<2:07"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8Ô∏è‚É£ Final Validation\n",
        "val_metrics = model.val(data=DATA_YAML)\n",
        "\n",
        "print(f\"\\nüìä Final Results:\")\n",
        "print(f\"  mAP50:     {val_metrics.box.map50:.4f}\")\n",
        "print(f\"  mAP50-95:  {val_metrics.box.map:.4f}\")\n",
        "print(f\"  Precision: {val_metrics.box.mp:.4f}\")\n",
        "print(f\"  Recall:    {val_metrics.box.mr:.4f}\")"
      ],
      "metadata": {
        "id": "ThRHsLkOgR5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 9Ô∏è‚É£ Test on Image\n",
        "from google.colab import files\n",
        "from IPython.display import display, Image as IPImage\n",
        "\n",
        "# Upload test image\n",
        "print(\"Upload an image to test:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load best model\n",
        "best_model = YOLO(f\"runs/detect/{run_name}/weights/best.pt\")\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    results = best_model(filename, save=True)\n",
        "    print(f\"\\nDetections in {filename}:\")\n",
        "    for box in results[0].boxes:\n",
        "        cls_id = int(box.cls[0])\n",
        "        conf = float(box.conf[0])\n",
        "        cls_name = CLASS_NAMES[cls_id] if cls_id < len(CLASS_NAMES) else str(cls_id)\n",
        "        print(f\"  - {cls_name}: {conf:.1%}\")\n",
        "\n",
        "    # Show result\n",
        "    result_path = list(Path(\"runs/detect/predict\").glob(f\"*{Path(filename).stem}*\"))[0]\n",
        "    display(IPImage(filename=str(result_path)))"
      ],
      "metadata": {
        "id": "y3j86kVwgZ3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üîü Download Model\n",
        "from google.colab import files\n",
        "\n",
        "# Download best weights\n",
        "files.download(f\"runs/detect/{run_name}/weights/best.pt\")\n",
        "\n",
        "# Optional: Export to ONNX and download\n",
        "# model.export(format=\"onnx\")\n",
        "# files.download(f\"runs/detect/{run_name}/weights/best.onnx\")"
      ],
      "metadata": {
        "id": "kILwIOGGgZ00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oaNiBZM-gZyG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}